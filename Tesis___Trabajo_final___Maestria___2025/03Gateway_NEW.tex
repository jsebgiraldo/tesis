\chapter{Elementos de la Arquitectura IoT para Smart Energy}

\section{Introducción}

Este capítulo presenta los elementos fundamentales de la arquitectura IoT propuesta para aplicaciones de Smart Energy, abarcando desde los nodos sensores de campo hasta las capacidades de procesamiento edge con inteligencia artificial. La arquitectura sigue un modelo jerárquico de tres niveles (nodos, routers y gateways) que permite escalabilidad masiva, eficiencia energética y resiliencia operativa, cumpliendo con los estándares IEEE 2030.5 (Smart Energy Profile 2.0) e ISO/IEC 30141 (IoT Reference Architecture).

La implementación propuesta integra tecnologías de conectividad de última generación (Thread 802.15.4, Wi-Fi HaLow 802.11ah), protocolos de aplicación optimizados para IoT (CoAP, LwM2M, MQTT) y capacidades de procesamiento edge mediante ThingsBoard Edge y modelos de lenguaje local (LLM). Los detalles técnicos de implementación (configuraciones UCI, docker-compose, scripts) se documentan en los anexos correspondientes.

\section{Visión General de la Arquitectura}

\subsection{Modelo Jerárquico de 3 Niveles IoT}

La arquitectura propuesta sigue un modelo jerárquico que permite desplegar redes IoT con miles de dispositivos manteniendo eficiencia operativa, optimizando la distribución de funciones, consumo energético y capacidad de procesamiento. Esta arquitectura, alineada con las implementaciones de referencia de Morse Micro para Wi-Fi HaLow y el ecosistema Thread de la Connectivity Standards Alliance, permite escalabilidad masiva en despliegues de Smart Energy.

Los tres niveles de la arquitectura son:

\begin{itemize}
    \item \textbf{Nivel 1 - Nodos IoT}: Dispositivos de campo con recursos limitados (sensores, actuadores, medidores inteligentes)
    \item \textbf{Nivel 2 - Routers Border}: Dispositivos intermedios que extienden cobertura y densifican la red mediante topologías mesh
    \item \textbf{Nivel 3 - Gateways Edge}: Plataformas de cómputo que agregan datos, ejecutan procesamiento edge y conectan con infraestructura WAN
\end{itemize}

Esta separación de funciones permite optimizar cada nivel según sus requisitos específicos de consumo energético, capacidad de procesamiento y conectividad, mientras mantiene interoperabilidad mediante protocolos estándares abiertos.

\subsection{Conformidad con Estándares Internacionales}

\subsubsection{IEEE 2030.5-2023 (Smart Energy Profile 2.0)}

El gateway implementa funcionalidades alineadas con IEEE 2030.5 (SEP 2.0), incluyendo los siguientes Function Sets:

\begin{itemize}
    \item \textbf{Device Capability (DCAP)}: Descubrimiento de capacidades (\texttt{/dcap})
    \item \textbf{Time (TM)}: Sincronización horaria NTP/PTP (<100 ms)
    \item \textbf{Metering Mirror (MM)}: Datos de medición con granularidad 15 min
    \item \textbf{Messaging (MSG)}: Notificaciones y alertas bidireccionales
    \item \textbf{End Device (ED)}: Registro y gestión de dispositivos
\end{itemize}

La seguridad IEEE 2030.5 se implementa mediante TLS 1.2/1.3 obligatorio, certificados X.509 ECC (curva P-256), LFDI derivado de certificado y RBAC para control de acceso. Los ejemplos completos de respuestas XML para todos los Function Sets se presentan en el \textbf{Anexo D}.

\subsubsection{ISO/IEC 30141:2024 (IoT Reference Architecture)}

El gateway implementa múltiples entidades funcionales según la vista funcional de ISO/IEC 30141: Sensing, Actuation, Processing, Storage, Communication, Security, Management y Application Support. La arquitectura cumple con las cuatro vistas del estándar (funcional, información, despliegue y operacional), proporcionando un marco completo para sistemas IoT industriales.

\subsection{Justificación del Modelo Jerárquico}

Ventajas de la arquitectura de 3 niveles: 

\textbf{(1) Escalabilidad masiva} - Un gateway gestiona 100-200 nodos directamente, escalando a 1000+ con routers intermedios mesh; 
\textbf{(2) Eficiencia energética} - Nodos transmiten en saltos cortos reduciendo potencia de transmisión, extendiendo autonomía con baterías a 5-10 años; 
\textbf{(3) Cobertura extendida} - HaLow alcanza >1 km en línea de vista, con routers mesh permite 3-5 km en entornos urbanos densos; 
\textbf{(4) Resiliencia operativa} - Topologías mesh reconfiguran rutas automáticamente ante fallos de enlaces o nodos; 
\textbf{(5) Distribución de carga} - Procesamiento distribuido reduce latencia y requisitos de ancho de banda WAN; 
\textbf{(6) Optimización de costos} - Infraestructura jerárquica reduce CAPEX/OPEX versus múltiples gateways independientes.

\section{Nivel 1: Nodos IoT (End Devices)}

Los nodos IoT constituyen la capa de campo de la arquitectura, implementando las funciones de sensing, actuation y comunicación de bajo consumo. En el contexto de Smart Energy, estos nodos pueden ser medidores inteligentes, sensores ambientales, actuadores para control de demanda o dispositivos de monitoreo de calidad de energía.

\subsection{Características Técnicas de Nodos}

Dispositivos sensores y actuadores de bajo consumo optimizados para operación con baterías durante años. Implementan Thread (802.15.4) o HaLow 802.11ah en modo cliente con protocolos LwM2M sobre CoAP, MQTT-SN o IEEE 2030.5 Client. 

\textbf{Especificaciones hardware típicas:}
\begin{itemize}
    \item MCU: Cortex-M4/M33 (ESP32-C6, nRF52840, STM32WB55)
    \item RAM: 256 KB - 1 MB
    \item Flash: 512 KB - 2 MB
    \item Radio: 802.15.4 (Thread) o 802.11ah (HaLow STA)
    \item Modos sleep profundo: <10 μA
    \item Autonomía: 5-10 años con batería AA (2500-3000 mAh)
\end{itemize}

\subsection{Protocolos de Comunicación en Nodos}

Los nodos implementan stacks de protocolos ligeros optimizados para dispositivos con recursos limitados:

\begin{itemize}
    \item \textbf{Thread 1.3}: IPv6 sobre 802.15.4 con routing mesh, comisionamiento seguro (PAKE), multicast confiable
    \item \textbf{CoAP (RFC 7252)}: Protocolo de aplicación request/response con observe pattern, block-wise transfers
    \item \textbf{LwM2M 1.2}: Framework de gestión de dispositivos sobre CoAP con modelo de objetos extensible (IPSO)
    \item \textbf{CBOR (RFC 8949)}: Serialización binaria compacta para payloads eficientes
\end{itemize}

La implementación de referencia de nodo ESP32-C6 con LwM2M se documenta en el \textbf{Anexo E}, incluyendo configuración de objetos IPSO para telemetría de energía, estrategias de sleep profundo y optimizaciones de consumo.

\section{Nivel 2: Routers Border IoT}

Los routers IoT extienden el alcance y densifican la cobertura de las redes de campo mediante topologías mesh, actuando como repetidores inteligentes sin capacidades de procesamiento edge ni gestión de dispositivos.

\subsection{Función de Routers en la Arquitectura}

Routers IoT que extienden el alcance de redes HaLow o Thread mediante mesh 802.11s, EasyMesh o Thread Router. Su función es puramente extensión de cobertura y densificación de red, sin procesamiento edge ni gestión de dispositivos. En despliegues de Smart Energy, estos routers se ubican estratégicamente en postes de alumbrado público, subestaciones secundarias o puntos de concentración de medidores.

\subsection{Especificaciones Técnicas de Routers}

\textbf{Hardware:}
\begin{itemize}
    \item SoC: MM8108 (HaLow) o nRF52840 (Thread)
    \item MPU: MIPS/ARM Cortex-A7 single-core para OpenWRT minimal
    \item RAM: 128-256 MB DDR3
    \item Flash: 32-64 MB NOR/NAND
    \item PoE: 802.3af/at (12.95W - 25.5W)
    \item Topología: Mesh 802.11s (HaLow) o Thread Router
\end{itemize}

\textbf{Software:}
\begin{itemize}
    \item Sistema operativo: OpenWRT 23.05 minimal (sin Docker)
    \item Funciones: Layer-2/Layer-3 forwarding, mesh path selection (HWMP), autenticación SAE
    \item Configuración: Remota mediante UCI batch o NETCONF
\end{itemize}

\subsection{Topologías Mesh y Algoritmos de Routing}

Los routers implementan algoritmos de routing mesh que optimizan métricas de calidad de enlace (LQI, RSSI, ETX) para seleccionar rutas óptimas dinámicamente. En HaLow, el protocolo HWMP (Hybrid Wireless Mesh Protocol, IEEE 802.11s) combina routing proactivo (rutas preestablecidas) y reactivo (on-demand), mientras que Thread utiliza el algoritmo MLE (Mesh Link Establishment) con selección de Parent basada en cost metrics.

\section{Nivel 3: Gateway de Borde (Border Router Edge)}

El gateway constituye el elemento de mayor capacidad de procesamiento en la arquitectura, actuando como puente entre las redes de campo (802.15.4/Thread, 802.11ah/HaLow) y las redes de área amplia (Ethernet, LTE/5G). Este componente implementa funciones avanzadas de agregación de datos, traducción de protocolos, seguridad end-to-end, resiliencia mediante buffering local y edge computing.

\subsection{Requisitos del Gateway}

\subsubsection{Requisitos Funcionales}

El gateway debe cumplir con: recepción de datos de $\geq$10 DCUs simultáneamente mediante 802.11ah, normalización OBIS/DLMS/COSEM a JSON/CBOR, publicación MQTT con QoS 1/2 garantizando entrega, buffer persistente local mínimo 7 días, uplink redundante Ethernet WAN (primario) + LTE M.2 (backup <30s), Access Point HaLow (902-928 MHz) con alcance mínimo 1 km, API REST IEEE 2030.5 compatible y entidades funcionales ISO/IEC 30141 completas.

\subsubsection{Requisitos No Funcionales}

Latencia E2E <5 segundos, disponibilidad >99.5\% con failover <30 seg, consumo energético <15W (LTE idle), operación -10°C a +50°C (Morse Micro: -40°C a +85°C), throughput HaLow mínimo 20 Mbps agregado, precisión sincronización <100 ms y soporte $\geq$250 EndDevices simultáneos.

\subsubsection{Requisitos de Seguridad}

Autenticación mutua TLS 1.2/1.3, certificados X.509 con renovación automática, Secure Boot, cifrado de credenciales, OTA segura con validación de firma digital, certificados ECC P-256 para IEEE 2030.5, LFDI derivado de certificado, RBAC para APIs REST y WPA3-SAE con PMF obligatorio en HaLow.

\subsection{Plataforma Hardware del Gateway}

\textbf{Especificaciones:}
\begin{itemize}
    \item Plataforma: Raspberry Pi 4 Model B (4 GB RAM)
    \item CPU: ARM Cortex-A72 quad-core @ 1.5 GHz (ARMv8-A 64-bit)
    \item Almacenamiento: NVMe SSD 128 GB (vía USB 3.0 bridge)
    \item Conectividad Thread: nRF52840 USB Dongle (RCP mode)
    \item Conectividad HaLow: Morse Micro MM6108 + MMP8000 + EVK (PCIe via USB)
    \item WAN: Gigabit Ethernet + Quectel RM502Q-AE LTE Cat-20 M.2
    \item Sistema operativo: OpenWRT 23.05.3 con kernel 5.15 LTS
\end{itemize}

La arquitectura ARM de 64 bits permite ejecutar contenedores Docker con ThingsBoard Edge, bases de datos TimescaleDB, brokers MQTT y modelos LLM locales (Ollama) con rendimiento adecuado para procesamiento edge en tiempo real.

\section{ThingsBoard Edge como Plataforma de Procesamiento}

\subsection{Visión General: Edge-First Architecture}

El gateway implementa una arquitectura centrada en **ThingsBoard Edge**, que actúa como plataforma de procesamiento edge completa, proporcionando capacidades de ingesta, transformación, almacenamiento, procesamiento de reglas (rule engine) y sincronización bidireccional con ThingsBoard Server en la nube. Esta arquitectura edge-first permite operación autónoma durante desconexiones WAN prolongadas (>72 horas) mientras mantiene funcionalidad completa de dashboards locales, alarmas y análisis en tiempo real.

ThingsBoard Edge cumple un rol fundamental en la arquitectura al actuar como middleware de integración entre los protocolos IoT de campo (CoAP, LwM2M, MQTT) y las aplicaciones de Smart Energy en la nube, implementando transformaciones de datos, procesamiento de eventos complejos (CEP) y almacenamiento persistente con TimescaleDB optimizado para series temporales.

\textbf{Flujo de Datos Multi-Protocolo:}

\begin{verbatim}
Nodos IoT                Gateway Edge                      Cloud/WAN
  (802.15.4)              (Raspberry Pi 4)                 (ThingsBoard Server)
     |                           |                                |
     | 6LoWPAN/CoAP              |                                |
     | (compressed IPv6)         |                                |
     +-------------------------->| OpenThread Border Router       |
                                 | (nRF52840 RCP)                 |
                                 |   ↓                            |
                                 | IPv6 routing (fd00::/64)       |
                                 |   ↓                            |
                                 | Bridge CoAP→MQTT               |
                                 | (descompresión 6LoWPAN)        |
                                 |   ↓                            |
                                 | ThingsBoard Edge               |
                                 | - Rule Engine (CEP)            |
                                 | - TimescaleDB (storage)        |
                                 | - Dashboards locales           |
                                 | - Alarmas en tiempo real       |
                                 |   ↓                            |
                                 | MQTT Publisher                 |
                                 | (payload comprimido CBOR)      |
                                 |   ↓                            |
                                 | HaLow 802.11ah                 |
                                 | (2/4/8 MHz bandwidth)          |
                                 +-------------------------------->| MQTT Broker
                                                                  | ThingsBoard Server
                                                                  | (sincronización gRPC)
\end{verbatim}

\subsection{Stack de Contenedores Docker}

El gateway despliega 7 servicios containerizados orquestados mediante Docker Compose, cada uno con responsabilidades específicas y aislamiento de recursos:

\subsubsection{1. OpenThread Border Router (OTBR)}

\textbf{Función:} Border router entre red Thread 802.15.4 (mesh IPv6) y red Ethernet del gateway, implementando traducción de direcciones IPv6, routing entre prefijos Thread (fd00::/64 mesh-local) y prefijos globales, y commissioning de nuevos dispositivos Thread.

\textbf{Implementación:}
\begin{itemize}
\item \textbf{Imagen}: \texttt{openthread/otbr:latest} (ARM64)
\item \textbf{Hardware}: nRF52840 USB Dongle como RCP (Radio Co-Processor) conectado vía \texttt{/dev/ttyACM0}
\item \textbf{Interfaces de red}: \texttt{wpan0} (Thread mesh), \texttt{eth0} (bridge a Ethernet)
\item \textbf{Servicios expuestos}: Web UI (puerto 80), mDNS/Avahi (auto-discovery), REST API Thread
\end{itemize}

\textbf{Configuración de Red Thread:}
\begin{verbatim}
Network Name: SmartGrid-Thread
PAN ID: 0xABCD
Channel: 15 (2.4 GHz, evita interferencia WiFi canales 1/6/11)
Network Key: [128-bit pre-shared key]
On-Mesh Prefix: fd00:db8:a0b:12f0::/64
\end{verbatim}

\textbf{Procesamiento 6LoWPAN:}

OTBR implementa descompresión automática de headers 6LoWPAN (IPHC/NHC) en la interfaz Thread, reconstruyendo paquetes IPv6 completos antes de rutearlos hacia la red Ethernet del gateway. Este proceso es transparente para aplicaciones, que ven tráfico IPv6 estándar:

\begin{enumerate}
\item Nodo Thread transmite paquete con headers comprimidos (3-9 bytes IPHC+NHC)
\item OTBR recibe en interfaz \texttt{wpan0}, descomprime headers a IPv6+UDP completos (48 bytes)
\item OTBR rutea paquete IPv6 a interfaz \texttt{eth0} (bridge Docker) hacia servicios locales
\item Bridge CoAP→MQTT (servicio 4) recibe paquete UDP/CoAP en puerto 5683
\end{enumerate}

\subsubsection{2. ThingsBoard Edge}

\textbf{Función:} Plataforma IoT edge completa que proporciona ingesta de telemetría, motor de reglas Complex Event Processing (CEP), almacenamiento de series temporales, dashboards interactivos locales, y sincronización bidireccional con ThingsBoard Server cloud.

\textbf{Implementación:}
\begin{itemize}
\item \textbf{Imagen}: \texttt{thingsboard/tb-edge:3.6.4} (Java/Spring Boot)
\item \textbf{Puertos}: 8080 (HTTP/WebSocket), 1883 (MQTT), 5683 (CoAP), 7070 (gRPC sync con cloud)
\item \textbf{Base de datos}: PostgreSQL + TimescaleDB (hypertables para telemetría)
\item \textbf{RAM asignada}: 4 GB (límite Docker), CPU: 3 cores (pinning para determinismo)
\end{itemize}

\textbf{Componentes Internos de ThingsBoard Edge:}

\begin{enumerate}
\item \textbf{Transport Layer}: Múltiples servidores de protocolo (MQTT, CoAP, HTTP, LwM2M) que reciben telemetría de dispositivos y publican comandos downlink.

\item \textbf{Rule Engine (Motor de Reglas CEP)}:
   \begin{itemize}
   \item \textbf{Rule Chains}: Grafos de nodos de procesamiento (filter, transformation, enrichment, action) que implementan lógica de negocio compleja.
   \item \textbf{Throughput}: >10,000 mensajes/seg con latencia <10 ms P99
   \item \textbf{Nodos disponibles}: Script (JS/Python), REST API Call, MQTT Publish, Alarm Create, Email, SMS, Webhook
   \item \textbf{Ejemplo Rule Chain Smart Energy}:
   \begin{verbatim}
   [MQTT Input] → [Script: Parse DLMS] → [Filter: consumption > 5kW]
                ↓                              ↓
   [TimescaleDB Save]              [Create Alarm: High Consumption]
                                              ↓
                                   [Email Notification to Customer]
   \end{verbatim}
   \end{itemize}

\item \textbf{Device Management}:
   \begin{itemize}
   \item Registro de dispositivos con atributos (ubicación, tipo, propietario)
   \item Gestión de credenciales (access tokens, X.509 certs)
   \item Grupos y relaciones (medidor → transformador → subestación)
   \item Firmware OTA via LwM2M Object 5
   \end{itemize}

\item \textbf{Data Storage - TimescaleDB Integration}:
   \begin{itemize}
   \item \textbf{Telemetría}: Hypertables con particionamiento automático por tiempo (chunks de 7 días)
   \item \textbf{Compresión columnar}: Reduce storage 10-20× para datos antiguos (>7 días)
   \item \textbf{Continuous Aggregates}: Vistas materializadas para agregaciones de 15-min, 1-hora, 1-día (actualizaciones incrementales)
   \item \textbf{Retención}: 90 días telemetría detallada, agregaciones 1-hora por 1 año, agregaciones 1-día indefinido
   \end{itemize}

\item \textbf{Dashboards Locales}:
   \begin{itemize}
   \item Widgets interactivos (gráficos de línea, gauges, mapas, tablas)
   \item Acceso local vía \texttt{http://<gateway-ip>:8080} durante offline
   \item Tiempo real con WebSocket (latencia <500 ms desde ingesta a visualización)
   \item Exportación de datos (CSV, JSON, Excel) para análisis offline
   \end{itemize}

\item \textbf{Alarm Engine}:
   \begin{itemize}
   \item Alarmas con severidades (Critical, Major, Minor, Warning, Indeterminate)
   \item Estados de alarma (Active, Acknowledged, Cleared)
   \item Propagación de alarmas (ej. falla de transformador propaga a todos medidores downstream)
   \item Notificaciones multi-canal (email, SMS, webhook, MQTT external)
   \end{itemize}
\end{enumerate}

\textbf{Sincronización Edge ↔ Cloud:}

ThingsBoard Edge implementa sincronización bidireccional sobre protocolo gRPC (puerto 7070/TLS):

\begin{itemize}
\item \textbf{Edge → Cloud (Uplink)}:
  \begin{itemize}
  \item Telemetría: Batches de 1,000 mensajes cada 5 min (modo online), batches de 5,000 con compresión gzip durante catch-up post-offline
  \item Alarmas: Inmediatas con prioridad alta (no se batchean)
  \item Atributos de dispositivos: Sincronización incremental cuando cambian
  \item Logs de auditoría: Eventos críticos (login, cambios de configuración)
  \end{itemize}

\item \textbf{Cloud → Edge (Downlink)}:
  \begin{itemize}
  \item Comandos RPC: Ejecución remota de acciones en dispositivos (corte/reconexión, actualización parámetros)
  \item Definiciones de dispositivos/assets: Sincronización automática de nuevos dispositivos registrados en cloud
  \item Actualizaciones de rule chains: Deploy remoto de nueva lógica de negocio
  \item Configuración de dashboards: Sincronización de cambios en visualizaciones
  \end{itemize}
\end{itemize}

\textbf{Modo Offline (Operación Autónoma):}

Durante desconexión WAN (detección: timeout gRPC >30s + ping fallido a 8.8.8.8):
\begin{enumerate}
\item ThingsBoard Edge continúa operación normal local (ingesta, rule engine, dashboards)
\item Mensajes se acumulan en queue persistente PostgreSQL + filesystem (\texttt{/var/lib/tb-edge/queue})
\item Capacidad de queue: 100,000 mensajes (~500 MB con compresión CBOR)
\item Política FIFO con priorización: Alarmas Critical > Alarmas Major > Telemetría > Logs
\item Dashboards locales permanecen accesibles vía LAN (\texttt{http://192.168.1.100:8080})
\item Alarmas se ejecutan localmente (notificaciones email solo si SMTP local configurado)
\end{enumerate}

Al recuperar conectividad WAN:
\begin{enumerate}
\item ThingsBoard Edge detecta reconexión (gRPC handshake exitoso)
\item Inicia catch-up sync acelerado: batch size 5,000 mensajes (vs 1,000 normal)
\item Prioriza alarmas pendientes (envío inmediato)
\item Comprime telemetría histórica con gzip (reducción 40-60\%)
\item Sincroniza backlog completo de 100k mensajes en ~10-15 minutos
\item Retorna a modo normal (batch 1,000, intervalo 5 min)
\end{enumerate}

\subsubsection{3. PostgreSQL + TimescaleDB}

\textbf{Función:} Base de datos relacional con extensión TimescaleDB para series temporales optimizadas, almacenando telemetría, configuración de dispositivos, alarmas, y usuarios de ThingsBoard Edge.

\textbf{Implementación:}
\begin{itemize}
\item \textbf{Imagen}: \texttt{timescale/timescaledb:2.13.0-pg15}
\item \textbf{Storage}: Volumen persistente en NVMe SSD (\texttt{/mnt/ssd/postgres-data})
\item \textbf{Configuración optimizada para IoT}:
  \begin{verbatim}
  shared_buffers = 1GB
  effective_cache_size = 3GB
  maintenance_work_mem = 256MB
  checkpoint_completion_target = 0.9
  wal_buffers = 16MB
  default_statistics_target = 100
  random_page_cost = 1.1 (SSD optimizado)
  effective_io_concurrency = 200
  work_mem = 16MB
  \end{verbatim}
\end{itemize}

\textbf{Hypertables para Telemetría:}
\begin{verbatim}
CREATE TABLE ts_kv (
  entity_id UUID NOT NULL,
  key VARCHAR(255) NOT NULL,
  ts BIGINT NOT NULL,
  bool_v BOOLEAN,
  str_v VARCHAR(10000),
  long_v BIGINT,
  dbl_v DOUBLE PRECISION,
  json_v JSON
);

SELECT create_hypertable('ts_kv', 'ts', chunk_time_interval => 604800000);
-- chunk_time_interval = 7 días en milisegundos
\end{verbatim}

\textbf{Políticas de Compresión y Retención:}
\begin{verbatim}
ALTER TABLE ts_kv SET (
  timescaledb.compress,
  timescaledb.compress_segmentby = 'entity_id,key',
  timescaledb.compress_orderby = 'ts'
);

SELECT add_compression_policy('ts_kv', INTERVAL '7 days');
SELECT add_retention_policy('ts_kv', INTERVAL '90 days');
\end{verbatim}

\subsubsection{4. Bridge CoAP→MQTT (Thread-ThingsBoard Integration)}

\textbf{Función:} Servicio custom Python que recibe mensajes CoAP/LwM2M desde nodos Thread (via OTBR), descomprime payloads, transforma a formato ThingsBoard JSON/CBOR, y publica vía MQTT local a ThingsBoard Edge.

\textbf{Implementación:}
\begin{verbatim}
# Dockerfile
FROM python:3.11-slim
RUN pip install aiocoap paho-mqtt cbor2
COPY bridge.py /app/
CMD ["python", "/app/bridge.py"]
\end{verbatim}

\textbf{Flujo de Procesamiento:}
\begin{enumerate}
\item \textbf{Recepción CoAP}: Servidor CoAP escucha puerto 5683/UDP, recibe mensajes de nodos Thread con IPs fd00::/64
\item \textbf{Descompresión 6LoWPAN}: Automática en OTBR (transparente para bridge)
\item \textbf{Parsing LwM2M}: Extrae Object/Instance/Resource IDs (ej. /3303/0/5700 = temperatura)
\item \textbf{Transformación a ThingsBoard}:
\begin{verbatim}
# Payload CoAP (LwM2M TLV binario):
[0xC8, 0x00, 0x14, 0x4C, 0x41, 0x37, 0x00, 0x00]  # Object 3303, Resource 5700, valor 23.5

# Transformación a ThingsBoard JSON:
{
  "ts": 1730409600000,
  "values": {
    "temperature": 23.5,
    "sensorId": "METER-001",
    "batteryLevel": 87
  }
}
\end{verbatim}
\item \textbf{Publicación MQTT}: Publica a topic \texttt{v1/devices/me/telemetry} con access token del dispositivo
\item \textbf{Manejo de errores}: Retry exponencial (1s, 2s, 4s, 8s) ante fallos MQTT, logging de mensajes perdidos
\end{enumerate}

\textbf{Código Simplificado del Bridge:}
\begin{verbatim}
import asyncio
import aiocoap
import paho.mqtt.client as mqtt
import cbor2
import json

class CoAPToMQTTBridge:
    def __init__(self):
        self.mqtt_client = mqtt.Client()
        self.mqtt_client.connect("localhost", 1883)
        
    async def coap_server(self):
        root = aiocoap.resource.Site()
        root.add_resource(['telemetry'], TelemetryResource(self))
        await aiocoap.Context.create_server_context(root, bind=('0.0.0.0', 5683))
        
    class TelemetryResource(aiocoap.resource.Resource):
        async def render_post(self, request):
            # Parse LwM2M TLV payload
            lwm2m_data = cbor2.loads(request.payload)
            device_id = request.remote.hostinfo  # IPv6 address
            
            # Transform to ThingsBoard format
            tb_payload = {
                "ts": int(time.time() * 1000),
                "values": {
                    "temperature": lwm2m_data['/3303/0/5700'],
                    "voltage": lwm2m_data['/3331/0/5700'],
                    "power": lwm2m_data['/3305/0/5800']
                }
            }
            
            # Publish to ThingsBoard Edge via MQTT
            topic = f"v1/devices/{device_id}/telemetry"
            self.bridge.mqtt_client.publish(topic, json.dumps(tb_payload))
            
            return aiocoap.Message(code=aiocoap.CHANGED)
\end{verbatim}

El código completo del bridge con manejo de errores, logging y métricas se documenta en el \textbf{Anexo C}.

\subsubsection{5. MQTT Publisher para HaLow}

\textbf{Función:} Cliente MQTT que consume mensajes procesados por ThingsBoard Edge (post rule-engine) y los transmite hacia ThingsBoard Cloud Server vía enlace HaLow 802.11ah, implementando compresión de payload, agregación de batches, y manejo de reconexiones ante inestabilidad del enlace.

\textbf{Implementación:}
\begin{itemize}
\item \textbf{Imagen}: Custom Python 3.11 con \texttt{paho-mqtt}, \texttt{cbor2}, \texttt{msgpack}
\item \textbf{Interfaz de salida}: \texttt{wlan2} (HaLow 802.11ah, rango IP 10.20.0.0/24)
\item \textbf{Servidor destino}: ThingsBoard Cloud Server (broker MQTT en \texttt{mqtt.thingsboard.cloud:1883}, puerto TLS 8883)
\item \textbf{QoS}: MQTT QoS 1 (at-least-once delivery) para garantizar entrega de telemetría crítica
\item \textbf{Persistencia}: Mensajes pendientes en SQLite local (\texttt{/mnt/docker/mqtt-publisher/queue.db})
\end{itemize}

\textbf{Proceso de Transmisión WAN:}

\begin{enumerate}
\item \textbf{Suscripción a TB Edge}:
   \begin{itemize}
   \item Se suscribe a topics internos de ThingsBoard Edge: \texttt{tb-edge/telemetry/\#}, \texttt{tb-edge/alarms/\#}
   \item Recibe mensajes post-procesamiento (con atributos enriquecidos, alarmas generadas)
   \end{itemize}

\item \textbf{Agregación y Compresión}:
   \begin{itemize}
   \item \textbf{Batching}: Agrupa hasta 100 mensajes de telemetría (ventana 30 segundos) en un solo payload
   \item \textbf{Compresión CBOR}: Convierte JSON a CBOR (reducción 30-40\% tamaño)
   \begin{verbatim}
   # Antes (JSON, 450 bytes):
   [{"ts":1730409600,"deviceId":"M001","temp":23.5,"voltage":230.1},
    {"ts":1730409605,"deviceId":"M002","temp":24.1,"voltage":229.8}, ...]
   
   # Después (CBOR, 280 bytes):
   [0x82, 0xA4, 0x62, 0x74, 0x73, 0x1B, ...]  # Array CBOR binario
   \end{verbatim}
   \item \textbf{Compresión gzip} (opcional, para batches >1 KB): Reducción adicional 40-60\%
   \item \textbf{Alarmas}: No se batchean, transmisión inmediata con QoS 2 (exactly-once)
   \end{itemize}

\item \textbf{Transmisión MQTT sobre HaLow}:
   \begin{itemize}
   \item Publica a topic cloud \texttt{v1/gateway/telemetry} con access token del gateway
   \item Configuración MQTT:
   \begin{verbatim}
   protocol: MQTTv5
   keepalive: 120 segundos (2 min, balanceado para HaLow)
   clean_session: False (sesión persistente ante desconexiones)
   max_inflight_messages: 20 (limita ventana TCP para BW limitado)
   reconnect_delay: 5-60 segundos (exponential backoff)
   \end{verbatim}
   \item \textbf{Binding a interfaz HaLow}: Fuerza uso de \texttt{wlan2} mediante socket option:
   \begin{verbatim}
   import socket
   sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
   sock.setsockopt(socket.SOL_SOCKET, socket.SO_BINDTODEVICE, b'wlan2')
   client.sock = sock
   \end{verbatim}
   \end{itemize}

\item \textbf{Manejo de Fallos HaLow}:
   \begin{itemize}
   \item \textbf{Detección de desconexión}: Timeout de keepalive MQTT (>2 min sin PINGRESP)
   \item \textbf{Queue persistente}: Mensajes no enviados se almacenan en SQLite (capacidad 10,000 mensajes)
   \item \textbf{Política de retry}:
     \begin{enumerate}
     \item Intento inmediato de reconexión (delay 5s)
     \item Si falla, espera 15s y reintenta
     \item Backoff exponencial: 30s, 60s, 120s (máx 2 min)
     \item Después de 10 intentos fallidos (20 min), activa notificación de alarma local
     \end{enumerate}
   \item \textbf{Failover a LTE}: Si HaLow no recupera en 30 min, switch automático a interfaz \texttt{wwan0} (LTE)
   \end{itemize}

\item \textbf{Monitoring de Throughput}:
   \begin{itemize}
   \item Métricas expuestas vía endpoint HTTP \texttt{/metrics} (formato Prometheus):
   \begin{verbatim}
   mqtt_messages_sent_total{interface="wlan2"} 45231
   mqtt_bytes_sent_total{interface="wlan2"} 12458672
   mqtt_publish_latency_seconds{quantile="0.99"} 0.85
   mqtt_reconnections_total{interface="wlan2"} 3
   mqtt_queue_depth_messages 0
   \end{verbatim}
   \item Alertas automáticas si:
     \begin{itemize}
     \item Latencia P99 > 2 segundos (congestión HaLow)
     \item Queue depth > 5,000 mensajes (desconexión prolongada)
     \item Reconnections > 10/hora (inestabilidad enlace)
     \end{itemize}
   \end{itemize}
\end{enumerate}

\textbf{Optimizaciones para Enlace HaLow (Limitado en Bandwidth):}

\begin{itemize}
\item \textbf{Downsampling adaptativo}: Si bandwidth HaLow cae <100 kbps (detección via throughput monitorizado), reduce frecuencia de telemetría:
  \begin{itemize}
  \item Normal: 1 mensaje/dispositivo/5 min → 300 msgs/hora para 100 dispositivos
  \item Modo degradado: 1 mensaje/dispositivo/15 min → 100 msgs/hora
  \item Priorización: Alarmas (100\% tasa) > Telemetría crítica (voltaje/corriente, 50\% tasa) > Telemetría periódica (temperatura, 10\% tasa)
  \end{itemize}

\item \textbf{Delta encoding}: Para variables que cambian lentamente (temperatura ambiente), transmite solo deltas:
\begin{verbatim}
# Mensaje inicial (completo):
{"ts":1730409600,"temp":23.5,"voltage":230.1,"current":4.5}  # 58 bytes JSON

# Mensajes subsecuentes (solo deltas):
{"ts":1730409900,"Δtemp":+0.3}  # 28 bytes JSON (50% reducción)
{"ts":1730410200,"Δtemp":-0.1}
{"ts":1730410500,"voltage":231.0,"Δcurrent":+0.2}  # Reset completo si delta acumulado > umbral
\end{verbatim}

\item \textbf{Compresión por diccionario}: Para campos repetitivos (deviceId, sensorType), usa diccionario compartido:
\begin{verbatim}
# Diccionario (enviado 1 vez al inicio de sesión MQTT):
{1: "deviceId", 2: "temperature", 3: "voltage", 4: "current", 5: "timestamp"}

# Mensaje comprimido:
{5:1730409600, 1:"M001", 2:23.5, 3:230.1}  # 30% menos bytes que claves string
\end{verbatim}

\item \textbf{Configuración TCP optimizada para HaLow}:
\begin{verbatim}
# Sysctl settings en contenedor MQTT Publisher
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_congestion_control = bbr  # Better Bandwidth & RTT
net.ipv4.tcp_notsent_lowat = 16384     # Limita buffer no enviado
net.core.rmem_max = 8388608
net.core.wmem_max = 4194304
net.ipv4.tcp_rmem = 4096 87380 4194304
net.ipv4.tcp_wmem = 4096 16384 2097152
\end{verbatim}
\end{itemize}

\textbf{Selección Adaptativa de Bandwidth HaLow:}

El sistema implementa cambio dinámico de bandwidth 802.11ah (2/4/8 MHz) basado en condiciones del enlace:

\begin{enumerate}
\item \textbf{Monitoring continuo}:
   \begin{itemize}
   \item Ejecuta cada 60 segundos: \texttt{iw dev wlan2 station dump}
   \item Extrae métricas: signal (RSSI), tx bitrate, tx failed, tx retries
   \end{itemize}

\item \textbf{Decisión de bandwidth}:
\begin{verbatim}
if RSSI < -85 dBm or tx_retry_rate > 30%:
    switch_to_2MHz()  # Mayor robustez, menor throughput
elif RSSI > -70 dBm and tx_retry_rate < 5%:
    switch_to_8MHz()  # Máximo throughput (hasta 40 Mbps)
else:
    switch_to_4MHz()  # Balanceado (hasta 10 Mbps)
\end{verbatim}

\item \textbf{Comando de cambio} (requiere desasociación/reasociación):
\begin{verbatim}
# Cambio a 2 MHz (mayor alcance):
uci set wireless.@wifi-iface[0].htmode='NOHT'     # Deshabilita HT (802.11n)
uci set wireless.@wifi-iface[0].bandwidth='2'
uci commit wireless
wifi reload

# Verificación:
iw dev wlan2 info | grep width  # Esperado: channel width: 2 MHz
\end{verbatim}

\item \textbf{Hysteresis}: Evita cambios frecuentes (flapping) manteniendo bandwidth al menos 5 minutos antes de permitir cambio.
\end{enumerate}

La arquitectura completa de sincronización HaLow con balanceo automático entre 2/4/8 MHz, incluyendo scripts de monitoring y cambio dinámico, se documenta en el \textbf{Anexo C}.

\subsubsection{6. Apache Kafka (Bus de Mensajes)}

\textbf{Función:} Bus de mensajes distribuido que desacopla productores (OTBR, TB Edge, sensores externos) de consumidores (reglas de ThingsBoard, LLM Ollama, servicios de análisis), proporcionando persistencia de mensajes, particionamiento para escalabilidad horizontal, y replicación para alta disponibilidad.

\textbf{Implementación:}
\begin{itemize}
\item \textbf{Imagen}: \texttt{confluentinc/cp-kafka:7.5.0} (versión Apache Kafka 3.5.1)
\item \textbf{Topics principales}:
  \begin{itemize}
  \item \texttt{telemetry.raw}: Datos crudos desde nodos (pre-procesamiento), 3 particiones
  \item \texttt{telemetry.processed}: Post rule-engine, listos para almacenamiento, 3 particiones
  \item \texttt{alarms.critical}: Alarmas prioritarias, 1 partición (ordenamiento garantizado)
  \item \texttt{commands.downlink}: Comandos hacia dispositivos, 2 particiones
  \end{itemize}
\item \textbf{Retención}: 7 días para telemetría (168h), 30 días para alarmas críticas
\item \textbf{Compresión}: snappy (balance velocidad/ratio, 30-40\% reducción)
\end{itemize}

\textbf{Integración con ThingsBoard Edge:}

ThingsBoard Edge 3.6 soporta Kafka como transport layer alternativo a MQTT interno:

\begin{verbatim}
# Configuración TB Edge para usar Kafka (tb-edge.yml):
queue:
  type: kafka
  kafka:
    bootstrap.servers: localhost:9092
    topic-properties:
      rule-engine: "tb-rule-engine"
      core: "tb-core"
      transport-api: "tb-transport-api"
      notifications: "tb-notifications"
    consumer-properties:
      group.id: tb-edge-consumer-group
      auto.offset.reset: earliest
      max.poll.records: 1000
\end{verbatim}

\textbf{Ventajas de Kafka en el Gateway:}

\begin{itemize}
\item \textbf{Desacoplamiento}: Rule engine puede procesar offline sin perder mensajes
\item \textbf{Replay}: Reprocesar mensajes históricos (últimos 7 días) para debugging o ajuste de reglas
\item \textbf{Múltiples consumidores}: Ollama LLM, exportadores Prometheus, scripts de análisis Python pueden consumir simultáneamente sin duplicar almacenamiento
\item \textbf{Backpressure}: Productores ralentizan automáticamente si consumidores no procesan (evita saturación RAM)
\item \textbf{Ordenamiento garantizado}: Mensajes de mismo \texttt{device\_id} en misma partición (ordenamiento por timestamp)
\end{itemize}

\subsubsection{7. Ollama LLM (Procesamiento de IA en Edge)}

\textbf{Función:} Motor de inferencia LLM local que ejecuta modelos Llama 2, Mistral o CodeLlama para análisis en tiempo real de patrones de consumo energético, detección de anomalías sin necesidad de conectividad cloud, y generación de respuestas a consultas en lenguaje natural sobre dashboards.

\textbf{Implementación:}
\begin{itemize}
\item \textbf{Imagen}: \texttt{ollama/ollama:0.1.38} (soporte ARM64/GPU)
\item \textbf{Modelo desplegado}: Mistral 7B quantized (Q4\_K\_M, ~4 GB RAM)
\item \textbf{Aceleración}: GPU VideoCore VI (Raspberry Pi 4, limitada) o CPU Cortex-A72 (4 threads)
\item \textbf{Latencia de inferencia}: 300-800 ms para prompts <500 tokens (dependiendo de carga CPU)
\item \textbf{RAM dedicada}: 4 GB límite Docker (suficiente para modelo 7B quantized + context)
\end{itemize}

\textbf{Casos de Uso de IA en Edge:}

\begin{enumerate}
\item \textbf{Detección de Anomalías en Consumo}:
   \begin{itemize}
   \item Input: Serie temporal de consumo últimos 7 días (agregaciones 15-min desde TimescaleDB)
   \item Prompt: "Analiza esta serie temporal de consumo energético e identifica patrones anómalos: [datos]"
   \item Output: JSON con anomalías detectadas, severidad, explicación
   \item Acción: Si anomalía Critical detectada, generar alarma automática en TB Edge
   \end{itemize}

\item \textbf{Predicción de Demanda (Próximas 24h)}:
   \begin{itemize}
   \item Input: Histórico consumo 30 días + metadatos (temperatura, día semana, feriados)
   \item Prompt: "Predice consumo energético próximas 24 horas basado en patrones históricos"
   \item Output: Array de 96 valores (intervalos 15-min) con bandas de confianza
   \item Acción: Enviar predicciones a dashboard "Forecast" para visualización
   \end{itemize}

\item \textbf{Chatbot Dashboard (Consultas NL)}:
   \begin{itemize}
   \item Input: Pregunta usuario en lenguaje natural ("¿Cuál fue el consumo máximo ayer?")
   \item Contexto: Acceso a API TimescaleDB para consultar datos reales
   \item Output: Respuesta textual + visualización sugerida (gráfico, tabla)
   \item Ejemplo:
   \begin{verbatim}
   Usuario: "Muéstrame medidores con consumo >5 kW en última hora"
   Ollama: [consulta SQL a TimescaleDB]
   Respuesta: "Se detectaron 12 medidores con consumo >5 kW:
              - METER-045: 6.2 kW (18:34)
              - METER-128: 5.8 kW (18:41)
              [...]
              ¿Deseas crear una alarma para monitorear estos medidores?"
   \end{verbatim}
   \end{itemize}
\end{enumerate}

\textbf{Integración Ollama ↔ ThingsBoard Edge:}

Se implementa mediante widget custom JavaScript en dashboard TB Edge que realiza llamadas HTTP a API Ollama:

\begin{verbatim}
// Widget JavaScript en TB Edge
async function queryOllama(prompt) {
  const response = await fetch('http://localhost:11434/api/generate', {
    method: 'POST',
    headers: {'Content-Type': 'application/json'},
    body: JSON.stringify({
      model: 'mistral:7b-q4',
      prompt: prompt,
      stream: false
    })
  });
  const data = await response.json();
  return data.response;
}

// Ejemplo de uso en Rule Chain:
// Nodo "Script Transformation" ejecuta consulta Ollama para cada mensaje
var telemetry = msg.power;  // 6.5 kW
if (telemetry > 5.0) {
  var aiResponse = queryOllama(
    "Explica por qué este consumo de " + telemetry + " kW es anómalo " +
    "comparado con histórico del medidor METER-045"
  );
  msg.alarmDetails = aiResponse;  // Adjunta explicación a alarma
}
return {msg: msg, metadata: metadata, msgType: msgType};
\end{verbatim}

Las configuraciones completas de Ollama, incluyendo ajuste de modelos, limitación de RAM, y ejemplos de prompts para casos de uso energéticos, se documentan en el \textbf{Anexo C}.

\subsection{Resumen del Stack Docker}

El gateway despliega 7 contenedores especializados:

\begin{enumerate}
\item \textbf{OTBR}: Border router Thread/802.15.4 → IPv6, descompresión 6LoWPAN automática
\item \textbf{ThingsBoard Edge}: Plataforma IoT completa (ingesta, rule engine, storage, dashboards, sync cloud)
\item \textbf{PostgreSQL + TimescaleDB}: Base de datos series temporales con compresión columnar y retención automática
\item \textbf{Bridge CoAP→MQTT}: Integrador Thread/LwM2M → ThingsBoard (transformación protocolos)
\item \textbf{MQTT Publisher HaLow}: Cliente MQTT con compresión CBOR, agregación batches, failover LTE, adaptación bandwidth 2/4/8 MHz
\item \textbf{Apache Kafka}: Bus de mensajes para desacoplamiento, replay, múltiples consumidores
\item \textbf{Ollama LLM}: Inferencia local Mistral 7B para detección anomalías, predicción demanda, chatbot NL
\end{enumerate}

Orquestación completa mediante Docker Compose con healthchecks, restart policies, y resource limits. Archivo \texttt{docker-compose.yml} completo en \textbf{Anexo B}.

\subsection{Stack de Comunicación}

Capa física: 802.15.4/Thread (RCP nRF52840 vía USB), 802.11ah HaLow (Morse Micro MM6108 vía SPI, 902-928 MHz, hasta 3 km, 40 Mbps), 802.11ac/ax WiFi dual-band, LTE Cat-6 M.2 y Ethernet Gigabit. Capa de red: IPv6 Thread (fd00::/64) ruteado por OTBR, IPv4 NAT para WAN. Capa de transporte: TCP/TLS (puerto 7070), MQTT/TLS (1883/8883), CoAP/UDP. Capa de aplicación: MQTT, HTTP/REST, WebSocket, JSON.

Las configuraciones de red UCI completas se documentan en el \textbf{Anexo F}.

\section{Implementación del Gateway con OpenWRT}

\subsection{Justificación de la Plataforma}

OpenWRT se selecciona por flexibilidad (Linux embebido con opkg/UCI), soporte Docker para contenedorización, redes avanzadas (VLAN, nftables, QoS, IPv6), amplio soporte de hardware con expansión de almacenamiento y comunidad activa con actualizaciones frecuentes.

\subsection{Hardware del Gateway}

\subsubsection{Plataforma Base}

Dos opciones: (1) Router industrial: SoC MediaTek MT7621AT (MIPS dual-core 880 MHz), RAM 512 MB DDR3, Flash 16 MB + USB 3.0/NVMe 32 GB, Ethernet 5 puertos Gigabit con PoE+; (2) Raspberry Pi 4 Model B: BCM2711 Cortex-A72 quad-core ARMv8 @ 1.5 GHz, 4 GB RAM, microSD 32 GB + M.2 NVMe SSD 256 GB via PCIe HAT, alimentación PoE+ HAT.

\subsubsection{Conectividad 802.11ah (HaLow) con Morse Micro}

Chipset MM6108 SoC con interfaz PCIe/SDIO/SPI, frecuencia 902-928 MHz con canales 1/2/4/8 MHz, alcance hasta 1-3 km LOS con antena externa 5 dBi, throughput hasta 40 Mbps (MCS10, 8 MHz BW), seguridad WPA3-SAE con PMF obligatorio. Ventajas Morse Micro: operación industrial -40°C a +85°C, drivers Linux mainline (ath11k), consumo <500 mW TX/<50 mW RX, certificaciones FCC/CE.

\textbf{Modos de Operación HaLow}: (1) AP (Access Point) - gateway como punto de acceso central; (2) STA (Station) - gateway como cliente conectado a AP externo; (3) 802.11s Mesh - malla autogestionada entre múltiples gateways con auto-healing; (4) EasyMesh - IEEE 1905.1 con roaming transparente y gestión centralizada.

Las configuraciones UCI completas para los cuatro modos HaLow, incluyendo ejemplos de verificación, pruebas de throughput y troubleshooting, se documentan en el \textbf{Anexo D}.

\section{Implementación en Raspberry Pi 4 con OpenWRT}

\subsection{Hardware de la Implementación Real}

El prototipo se implementó sobre Raspberry Pi 4 Model B por sus capacidades multi-core y memoria RAM esenciales para múltiples contenedores Docker. Justificación vs Router MT7621AT: 4 núcleos Cortex-A72 permiten paralelización sin contención, 4 GB RAM suficientes para PostgreSQL/Kafka/TB Edge, ecosistema ARM64 con imágenes Docker oficiales, PCIe para NVMe con >3000 IOPS crítico para PostgreSQL, GPIO/SPI flexible.

\subsubsection{Periféricos y Módulos de Conectividad}

(1) \textbf{Thread}: Nordic nRF52840 Dongle con firmware OpenThread RCP v1.3, interfaz USB 2.0 (\texttt{/dev/ttyACM0}), potencia TX +8 dBm, sensibilidad -95 dBm; (2) \textbf{HaLow}: Morse Micro MM6108 vía SPI0 (GPIO 8/9/10/11/25), driver \texttt{ath11k} mainline, identificación \texttt{wlan2}; (3) \textbf{LTE}: Quectel BG95-M3 (Cat-M1/NB-IoT + EGPRS), interfaz USB (\texttt{wwan0}), throughput 375 kbps, latencia 100-300 ms; (4) \textbf{Almacenamiento}: Kingston NV2 M.2 NVMe 256 GB via PCIe HAT (350-400 MB/s lectura, 3200-3500 IOPS 4K random); (5) \textbf{Alimentación}: Waveshare PoE HAT IEEE 802.3at (25.5W máx), salida 5V/5A, ventilador PWM (encendido T°>60°C).

La conexión SPI del módulo HaLow, habilitación en OpenWRT y verificación de interfaz se documentan en el \textbf{Anexo F}.

\subsection{Sistema Operativo: OpenWRT 23.05 en Raspberry Pi 4}

OpenWRT 23.05.0, target \texttt{bcm27xx/bcm2711} (ARMv8 64-bit), kernel Linux 5.15.134 LTS, arquitectura binarios \texttt{aarch64\_cortex-a72}, libc musl 1.2.4. Los procedimientos completos de instalación (descarga, escritura en microSD, configuración inicial, actualización de paquetes, configuración de almacenamiento NVMe con \texttt{fstab}, directorios Docker) se documentan en el \textbf{Anexo A}.

\subsection{Configuración de Conectividad}

El gateway integra múltiples interfaces: Thread 802.15.4 (OTBR con nRF52840 RCP formando red SmartGrid-Thread en canal 15), HaLow 802.11ah (MM6108 vía SPI soportando 4 modos: AP Router con NAT, STA Client, Mesh 802.11s con HWMP routing, EasyMesh 1905.1 con Controller/Agent), LTE Cat-M1/NB-IoT (Quectel BG95-M3 con failover automático vía mwan3) y Ethernet Gigabit (WAN primaria).

\textbf{Ejemplo de verificación de interfaces activas}:
\begin{verbatim}
# Thread Border Router
docker exec otbr ot-ctl state  # Esperado: "leader" o "router"

# HaLow 802.11ah
iw dev wlan2 info  # Esperado: type AP, channel 7 (917 MHz)

# LTE modem
mmcli -m 0 --simple-status  # Esperado: state: connected

# Ethernet WAN
cat /sys/class/net/eth0/operstate  # Esperado: up (1000BASE-T)
\end{verbatim}

Las configuraciones UCI completas para HaLow en sus cuatro modos de operación se presentan en el \textbf{Anexo D}.

\section{Flujo de Datos End-to-End}

\subsection{Flujo Normal de Operación}

Medidor → Nodo Thread (ESP32C6) vía RS-485/DLMS → OTBR (ruteo IPv6 desde \texttt{fd00::/64} a LAN) → Bridge (transformación CoAP/MQTT → formato ThingsBoard JSON) → TB Edge (procesamiento Rule Engine, almacenamiento PostgreSQL, actualización dashboards) → TB Cloud (sincronización gRPC/TLS puerto 7070 cada 5 min) → Visualización dashboards.

El flujo inverso para comandos downlink sigue: TB Cloud → TB Edge (validación permisos RBAC) → Bridge (traducción a protocolo nodo LwM2M Write / IEEE 2030.5 DER Control) → Routers mesh (reenvío) → Nodo (ejecución + ACK).

\subsection{Flujo en Modo Edge (Sin Conectividad Cloud)}

Gateway detecta pérdida WAN (ping a \texttt{8.8.8.8} falla), TB Edge activa modo offline continuando operación local (reglas, dashboards accesibles via LAN), datos se acumulan en queue persistente PostgreSQL + filesystem (límite 100k msgs o 2 GB), al recuperar conectividad sincroniza automáticamente backlog completo en ~10-15 minutos con batch size 5000 y compresión gzip.

\subsection{Flujo de Actualización OTA de Contenedores}

Watchtower container verifica actualizaciones de imágenes Docker cada 24h, si nueva versión disponible descarga imagen, detiene contenedor actual, crea nuevo con misma configuración (volúmenes, redes), si healthcheck OK elimina imagen antigua, si falla rollback automático a imagen anterior. Logs de actualización en \texttt{/mnt/docker/watchtower/watchtower.log}.

\section{Arquitectura de Datos: Kafka y PostgreSQL}

\subsection{Integración de Apache Kafka}

Kafka proporciona message broker distribuido de alto rendimiento: intermedia entre bridge (productor) y TB Edge (consumidor), buffer distribuido con tópicos persistentes (telemetry, alarms), soporta >100k msg/s con múltiples particiones, retención configurable (7 días default). Ventajas vs in-memory queue: capacidad GB vs 100k msgs, replay histórico desde offset específico, multi-consumidor (TB Edge + analítica + ML simultáneamente), backpressure absorption sin pérdida de mensajes.

El docker-compose completo de Kafka (Zookeeper + Kafka broker) y scripts Python para productor/consumidor se documentan en \textbf{Anexo B} y \textbf{Anexo C}.

\subsection{PostgreSQL + TimescaleDB}

PostgreSQL con extensión TimescaleDB almacena: telemetría histórica (series temporales optimizadas con compresión 10-20×, particionamiento automático por tiempo en chunks de 7 días, aggregaciones rápidas con \texttt{time\_bucket}), configuración de dispositivos (atributos, credenciales, relaciones), alarmas/eventos (log persistente para auditoría) y dashboards/reglas de TB Edge. 

El esquema completo de TimescaleDB incluyendo definición de hypertables, políticas de compresión, continuous aggregates (vistas materializadas para agregaciones de 15-min, 1-hora y 1-día), políticas de retención (90 días) y cinco consultas SQL de ejemplo se presenta en el \textbf{Anexo D}.

\section{Protocolos de Comunicación IoT}

El gateway implementa múltiples protocolos según caso de uso:

\begin{itemize}
    \item \textbf{MQTT (QoS 0/1/2)}: Telemetría uplink (medidor→gateway), patrón Pub/Sub desacoplado, QoS garantizado (QoS 1 at least once, QoS 2 exactly once), Last Will Testament para detección de desconexión, retained messages para último valor, broker Mosquitto local con TLS/mTLS
    \item \textbf{CoAP (UDP)}: Thread mesh intra-nodo, overhead 4 bytes vs 100+ HTTP, Observe para suscripciones, DTLS+PSK para seguridad, block-wise transfer para mensajes >1024 bytes, métodos RESTful (GET/POST/PUT/DELETE)
    \item \textbf{HTTP/REST}: APIs gestión (TB Edge puerto 8080, IEEE 2030.5 puerto 8883, LuCI puerto 80, Ollama puerto 11434), webhooks para integraciones, consultas cloud
    \item \textbf{LwM2M}: Device management (bootstrap, firmware OTA), objetos estándar OMA SpecWorks (Security 0, Server 1, Device 3, Connectivity 4, Firmware Update 5), operaciones Read/Write/Execute/Observe/Discover, transporte CoAP sobre UDP (binding U) o SMS/NB-IoT (binding S), DTLS eficiente (PSK 16 bytes vs X.509 2 KB)
\end{itemize}

La selección de protocolo por caso de uso se documenta en tabla comparativa en el documento original. La implementación completa de referencia de un nodo IoT ESP32-C6 con cliente LwM2M AVSystems Anjay se documenta en el \textbf{Anexo E}.

\section{Resiliencia y Almacenamiento Persistente}

\subsection{Arquitectura de Almacenamiento}

Estrategia de almacenamiento de alta resiliencia: Flash interna 128 MB (sistema OpenWRT + configuración UCI), SSD M.2 NVMe 256 GB (datos persistentes Docker/PostgreSQL/queue TB Edge), USB 3.0 opcional (backups periódicos). Ventajas SSD NVMe vs microSD/USB: durabilidad >1M ciclos E/W (MTBF >1.5M horas), desempeño >3000 IOPS escritura (latencia <0.1ms vs 5-20ms SD), fiabilidad con ECC interno, power-loss protection (PLP) y SMART monitoring.

\subsection{ThingsBoard Edge Queue: Resiliencia Offline}

TB Edge implementa cola de mensajes persistente garantizando resiliencia ante pérdida de conectividad cloud. Arquitectura: queue storage en PostgreSQL + filesystem (\texttt{/mnt/ssd/docker/queue}), capacidad hasta 100k mensajes (~500 MB CBOR), política FIFO con priorización de alarmas críticas sobre telemetría histórica.

\textbf{Modo Online (conectividad cloud activa)}: TB Edge sincroniza cada 5 minutos batch de 1000 mensajes con TB Cloud vía gRPC (puerto 7070), al confirmar ACK elimina mensajes de la cola.

\textbf{Modo Offline (sin conectividad cloud)}: TB Edge detecta pérdida de conexión (timeout gRPC >30s), cambia a modo offline continuando procesamiento local, mensajes se acumulan en queue persistente, dashboards locales permanecen funcionales (\texttt{http://<gateway-ip>:8080}), alarmas se ejecutan localmente, queue crece hasta límite configurado (100k msgs o 2 GB).

\textbf{Recuperación de Conectividad (catch-up sync)}: TB Edge detecta reconexión (gRPC handshake exitoso), inicia sincronización acelerada con batch size 5000 mensajes, prioriza alarmas/eventos críticos, comprime datos con gzip (40-60\% reducción), sincroniza backlog completo de 100k msgs en ~10-15 minutos, retorna a modo normal (batch 1000, intervalo 5 min).

\textbf{Protección contra Desbordamiento}: Script de monitoreo ejecutado vía cron cada hora elimina telemetría histórica >7 días, comprime eventos no críticos con gzip, notifica operador si queue >1.8 GB (90\% del límite).

La configuración completa de queue (archivo \texttt{tb-edge.yml} con parámetros de sync\_interval, batch\_size, compression, retry\_policy, persistent\_queue) y scripts de monitoreo se documentan en \textbf{Anexo B} y \textbf{Anexo C}.

\subsection{Resiliencia Multinivel}

Seis niveles de resiliencia con Recovery Time Objective (RTO): L1 Hardware (SSD NVMe con ECC/PLP/SMART, RTO 0s), L2 Filesystem (ext4 con journaling/fsck automático, RTO <30s), L3 Base de datos (PostgreSQL WAL/autovacuum/replication slots, RTO <60s), L4 Aplicación (TB Edge Queue con persistent queue/retry policy/compression, RTO <300s), L5 Red (mwan3 WAN failover Ethernet primario/LTE backup con tracking activo, RTO <30s), L6 Container (Docker healthchecks/restart policy/Watchtower auto-updates, RTO <120s).

\section{Gestión Remota del Gateway}

\subsection{Feeds de OpenWRT}

OpenWRT utiliza feeds (repositorios de paquetes) para extender funcionalidad: feeds oficiales (base, packages, luci, routing, telephony con >10k paquetes) + feeds custom para aplicaciones propietarias Smart Grid. Gestión con opkg: \texttt{opkg update}, \texttt{opkg find}, \texttt{opkg install}, \texttt{opkg upgrade}, \texttt{opkg list-installed}.

La configuración de feeds custom incluyendo estructura de directorios, ejemplo de Makefile para paquete personalizado (\texttt{tb-edge-connector}) y hosting vía nginx se documenta en el \textbf{Anexo F}.

\subsection{OpenVPN: Acceso Remoto Seguro}

OpenVPN proporciona túnel VPN cifrado para gestión remota: acceso SSH seguro desde NOC, LuCI web UI sin exponer puerto 80/443 a internet, debugging remoto (logs, tcpdump, análisis performance), túnel permanente hub-spoke. Arquitectura: NOC Server VPN (10.8.0.0/24) → Gateway 1 (10.8.0.100) / Gateway 2 (10.8.0.101) / ... / Gateway N (10.8.0.199) + Admin PC (10.8.0.50).

Configuración cliente OpenVPN: certificados PKI (ca.crt, gateway-001.crt, gateway-001.key, ta.key), compresión lzo adaptive, keepalive 10/120 (detectar desconexión en 120s), persistencia de túnel, logging, pull routes desde servidor, reconexión automática, usuario sin privilegios (nobody/nogroup).

Configuración servidor VPN: puerto 1194 UDP, certificados (ca/server/dh2048), client-to-client (permitir gateways comunicarse), push routes a clientes (red NOC 10.10.0.0/24), keepalive, logging, client-config-dir (CCD) para IPs fijas por gateway y push de rutas específicas.

Las configuraciones completas UCI (\texttt{/etc/config/openvpn}), archivos .conf y CCD se documentan en el \textbf{Anexo F}.

\subsection{OpenWISP: Gestión Centralizada de Gateways}

OpenWISP es plataforma open-source para gestión masiva (100-1000 gateways): Controller Django (backend), Config agente en gateway, Monitoring (colección de métricas CPU/RAM/tráfico), Firmware Upgrader (actualizaciones OTA masivas), Network Topology (visualización). 

Funcionalidades: templates UCI con variables (\texttt{\{\{apn\}\}}, \texttt{\{\{halow\_channel\}\}}), push configuración remota vía HTTPS con aplicación automática (\texttt{uci commit \&\& reload\_config}), actualizaciones OTA programadas (inmediata o ventana de mantenimiento 3 AM) con actualización segura dual-partition (escribir Partition B, reiniciar, si falla rollback automático a Partition A), monitoreo de uptime/CPU/RAM/storage/interfaces/Docker, alertas configurables (email/SMS/webhook) para Gateway Offline, High CPU, Low Disk, LTE Failover.

La instalación completa de OpenWISP Config en gateway, despliegue de OpenWISP Controller en Docker (docker-compose.yml con PostgreSQL/Redis/Dashboard/Celery), gestión de configuraciones con templates JSON, firmware OTA workflow y configuración de alertas se documentan en el \textbf{Anexo F}.

\subsection{Comparación de Herramientas de Gestión}

LuCI (local) para gestión individual sin gestión masiva, OpenVPN+SSH para <10 gateways con CLI manual, OpenWISP completo para 100-10,000 gateways con templates/push automático/Firmware OTA scheduler/monitoring/alertas/zero-touch provisioning, todo open-source (\$0).

\section{Gestión de Uplink Redundante (Ethernet + LTE)}

\subsection{Política de Failover Automático}

OpenWRT implementa failover basado en route metrics: Ethernet WAN metric=10 (prioridad alta), LTE metric=20 (backup). Kernel selecciona ruta con menor métrica (Ethernet), si falla (link down) cambia automáticamente a LTE, al recuperar Ethernet restaura ruta principal, tiempo de conmutación <30 segundos incluyendo renegociación TCP.

Las configuraciones UCI de interfaces \texttt{wan\_eth} y \texttt{wan\_lte} con protocolo dhcp/modemmanager y métricas se documentan en el \textbf{Anexo F}.

\subsection{Monitoreo Activo de Conectividad (mwan3)}

Paquete mwan3 proporciona tracking proactivo de enlaces WAN: ping periódico a \texttt{8.8.8.8} y \texttt{1.1.1.1}, reliability de 2 pings perdidos para declarar fallo (failover), count 3 / timeout 2 / interval 5, políticas de balanceo (75\% Ethernet / 25\% LTE), reglas específicas por servicio (MQTT puerto 8883 solo por Ethernet). Verificación con \texttt{mwan3 status} y \texttt{mwan3 interfaces}.

La configuración completa de mwan3 (\texttt{/etc/config/mwan3} con interfaces, policies, rules) se documenta en el \textbf{Anexo F}.

\subsection{Optimización de Costos LTE}

Estrategias para minimizar consumo celular: (1) Compresión CBOR vs JSON (reducción 40-60\% en tamaño payload); (2) Batching - TB Edge acumula 5 min de telemetría y envía en un solo paquete HTTP/2; (3) Compresión gzip para payloads >1 KB; (4) Políticas de tráfico por WAN - script hotplug \texttt{/etc/hotplug.d/iface/99-wan-monitor} detecta si LTE activo y adapta comportamiento (detener Watchtower, aumentar intervalo sync TB Edge de 5 min a 1h); (5) Monitoreo consumo con vnstat (\texttt{vnstat -m -i wwan0}), alarma si >10 GB/mes deshabilitando LTE y enviando alerta a TB Edge.

Los scripts hotplug \texttt{99-wan-monitor} y \texttt{check-lte-quota.sh} se documentan en el \textbf{Anexo C}.

\section{Gestión y Monitoreo del Gateway}

\subsection{Interfaz de Gestión (LuCI)}

LuCI proporciona interfaz web en \texttt{http://<gateway-ip>:80} con módulos: Network (configuración interfaces WAN/LAN, WiFi, firewall, DHCP), System (estado CPU/RAM/storage, logs, backups), Docker (gestión contenedores vía luci-app-dockerman: start/stop, logs, stats), Services (configuración servicios dnsmasq, dropbear SSH, uhttpd).

\subsection{Monitoreo de Contenedores}

Docker stats para visualización en tiempo real de CPU\%/MEM USAGE/MEM\%/NET I/O por contenedor con \texttt{docker stats --no-stream}. Healthchecks en docker-compose.yml: test (\texttt{curl -f http://localhost:8080/api/health}), interval 30s, timeout 10s, retries 3, start\_period 120s. Verificación con \texttt{docker ps --filter "health=unhealthy"}.

\subsection{Logs Centralizados}

Consulta logs por contenedor con \texttt{docker logs -f --tail=100 tb-edge} o \texttt{docker logs --since 1h otbr | grep ERROR}. Syslog integration: configurar log-driver syslog en \texttt{/etc/docker/daemon.json} para enviar a servidor remoto UDP 514 con tag \texttt{gateway-{{.Name}}}.

\subsection{Backups y Recuperación}

Backup OpenWRT vía LuCI (System > Backup/Flash Firmware > Generate archive) o CLI \texttt{sysupgrade -b /tmp/backup-\$(date +\%Y\%m\%d).tar.gz}. Backup volúmenes Docker con script diario ejecutado vía cron (\texttt{0 2 * * *}): tar czf de \texttt{tb-edge-data}, \texttt{postgres-data}, \texttt{otbr-config}, retención 7 días (find -mtime +7 -delete).

Disaster recovery: restaurar OpenWRT (flash imagen + restaurar backup configuración), montar volumen de datos (\texttt{mount /dev/sda1 /mnt/docker}), restaurar volúmenes desde backup si necesario, desplegar contenedores (\texttt{docker-compose up -d}), verificar healthchecks (\texttt{docker ps}), sincronizar TB Edge con cloud (automático al conectar).

Los scripts de backup automatizado \texttt{backup.sh} se documentan en el \textbf{Anexo C}.

\section{Pruebas y Validación}

\subsection{Pruebas Funcionales}

Validaciones clave: (1) Formación red Thread - verificar OTBR leader/router con \texttt{docker exec otbr ot-ctl state} y \texttt{ot-ctl child table}; (2) Conexión HaLow - asociación DCUs con \texttt{iw dev wlan2 station dump}, señal >-70 dBm, throughput >20 Mbps con iperf3; (3) Validación 4 modos HaLow - AP con \texttt{hostapd\_cli all\_sta}, STA con \texttt{iw link}, Mesh 802.11s con \texttt{iw mpath dump} y test multi-hop ping6, EasyMesh con \texttt{ubus call map.controller dump\_topology} y test roaming/band steering; (4) Failover Ethernet/LTE - \texttt{ifdown wan\_eth}, verificar \texttt{mwan3 status}, reconectar; (5) Publicación MQTT con \texttt{mosquitto\_pub}, sincronización cloud con \texttt{docker logs tb-edge | grep "Cloud synchronization"}, comando downlink.

\subsection{Pruebas de Desempeño}

Latencia E2E objetivo <5s percentil 95 con timestamps en payload + análisis en TB Edge. Throughput HaLow: 10 DCUs @ 2 Mbps = 20 Mbps agregado, pérdida <0.1\% con señal >-65 dBm, rango verificar conectividad 1 km LoS y 500 m NLOS. Throughput MQTT: 10 dispositivos publicando cada 15 seg = 40 msg/min, escalar hasta observar pérdida o latencia >5s. Consumo energético con PoE meter: idle <5W, carga media <12W, carga alta <18W (límite PoE+ 25W). Resiliencia offline: 24h sin WAN, buffer >28k mensajes (300 medidores × 96 lecturas/día), sincronización completa <10 min al reconectar. Tiempo failover WAN: ping continuo a \texttt{8.8.8.8}, objetivo <30 segundos.

\subsection{Pruebas de Seguridad}

Validaciones: (1) Firewall - escaneo \texttt{nmap -sS -p- <gateway-wan-ip>}, esperado solo puertos explícitos (22 SSH, 443 HTTPS); (2) HaLow WPA3-SAE - validar \texttt{iw dev wlan2 info | grep PMF} esperado "PMF: required", intentar asociación con estación WPA2-only rechazada; (3) TLS/mTLS - \texttt{openssl s\_client -connect <tb-cloud>:7070 -CAfile ca.crt}, verificar return code 0; (4) Inyección MQTT - \texttt{mosquitto\_pub -h localhost -p 1883 -t test -m "unauthorized"}, esperado Connection refused; (5) Container escape - \texttt{docker inspect tb-edge | grep '"Privileged": false'} excepto OTBR; (6) LTE APN security - \texttt{grep -r "apn.*password" /var/log/}, esperado sin resultados; (7) Actualizaciones automáticas - \texttt{docker logs watchtower | grep "Updated"}.

\subsection{Pruebas de Integración}

Comisionado Thread vía OTBR web UI, reglas TB Edge con alarmas (crear regla consumo >5 kW, verificar activación), dashboard en tiempo real con latencia <2s, API REST consultas (\texttt{curl -X GET http://localhost:8080/api/tenant/devices -H "X-Authorization: Bearer \$TOKEN"}), resiliencia offline 24h con generación de 28,800 mensajes, verificar queue size ~150-200 MB con compresión, reconectar WAN, monitorear catch-up sync esperando 100k msgs sincronizados en <15 min.

\section{Integración de Inteligencia Artificial con MCP y LLM}

\subsection{Motivación: IA en el Edge para Smart Energy}

La integración de capacidades de inteligencia artificial directamente en los gateways de borde representa un cambio de paradigma en la gestión de redes eléctricas inteligentes. Tradicionalmente, el análisis avanzado de datos de medición se realizaba exclusivamente en infraestructura centralizada en la nube, lo que introduce dependencias críticas de conectividad WAN, latencias significativas (2-5 segundos) y costos recurrentes de transferencia de datos. Además, el envío de datos de consumo energético a servicios cloud externos plantea preocupaciones de privacidad y cumplimiento regulatorio (GDPR, CCPA, Ley 1581 de 2012 en Colombia).

El procesamiento de IA en el edge (gateway local) ofrece ventajas fundamentales para aplicaciones de Smart Energy:

\begin{itemize}
    \item \textbf{Latencia reducida}: Análisis en <500 ms vs 2-5 segundos en cloud, crítico para detección de fraude en tiempo real
    \item \textbf{Privacidad y soberanía de datos}: Información sensible de consumo nunca abandona el perímetro del gateway, cumpliendo normativas de protección de datos
    \item \textbf{Disponibilidad offline}: Capacidades analíticas mantienen operación durante desconexiones WAN prolongadas (>72 horas)
    \item \textbf{Reducción de costos}: Eliminación de cargos por API calls a servicios cloud (\$0.01-0.10 por consulta) y reducción de tráfico WAN
    \item \textbf{Escalabilidad distribuida}: Cada gateway procesa su zona de cobertura (100-250 medidores) sin congestionar infraestructura centralizada
\end{itemize}

Sin embargo, la integración de modelos de lenguaje (LLM) y sistemas de IA en gateways IoT presenta desafíos arquitectónicos significativos: (1) recursos computacionales limitados (CPU ARM, 4-8 GB RAM), (2) necesidad de acceso estructurado a datos de telemetría y configuración, (3) complejidad de mantener código de integración custom entre cada LLM y cada plataforma IoT, (4) riesgo de acoplamiento fuerte entre componentes que dificulta actualizaciones y mantenimiento.

\subsection{Model Context Protocol (MCP): Estandarización de Integraciones de IA}

Model Context Protocol (MCP) es un protocolo de comunicación estándar abierto desarrollado por Anthropic que resuelve el problema de integración entre aplicaciones y servicios de inteligencia artificial mediante una arquitectura desacoplada basada en herramientas (tools), recursos (resources) y prompts estructurados. MCP establece una interfaz uniforme que permite a cualquier modelo de lenguaje (Claude, GPT-4, Llama, Mistral, Phi-3) acceder a datos y ejecutar acciones en sistemas externos sin necesidad de código de integración específico para cada combinación modelo-plataforma.

\subsubsection{Arquitectura Conceptual de MCP}

La arquitectura MCP se compone de tres elementos fundamentales:

\textbf{1. MCP Server} - Componente que expone capacidades de un sistema backend (ThingsBoard Edge, bases de datos, APIs) al ecosistema de IA mediante:
\begin{itemize}
    \item \textbf{Tools}: Funciones invocables por el LLM (ej. \texttt{get\_device\_telemetry}, \texttt{create\_alarm}, \texttt{update\_device\_attributes})
    \item \textbf{Resources}: Fuentes de datos contextuales (ej. esquemas de dispositivos, configuraciones, documentación)
    \item \textbf{Prompts}: Plantillas de consulta predefinidas para casos de uso comunes
\end{itemize}

\textbf{2. MCP Client} - Aplicación que consume servicios de IA y coordina la comunicación entre el usuario, el LLM y los MCP Servers. El cliente mantiene el contexto de la conversación, gestiona múltiples conexiones a MCP Servers y presenta resultados al usuario (dashboard, chatbot, API REST).

\textbf{3. Protocolo de Comunicación} - MCP utiliza JSON-RPC 2.0 como formato de mensajes, soportando múltiples transportes:
\begin{itemize}
    \item \textbf{stdio}: Comunicación por entrada/salida estándar (ideal para procesos locales)
    \item \textbf{Server-Sent Events (SSE)}: Streaming HTTP para conexiones remotas
    \item \textbf{WebSocket}: Comunicación bidireccional full-duplex para aplicaciones interactivas
\end{itemize}

\subsubsection{Flujo de Interacción MCP en el Gateway}

El flujo típico de una consulta de análisis con MCP integrado en el gateway es:

\begin{verbatim}
Usuario → MCP Client → LLM → MCP Server → ThingsBoard Edge API → Respuesta
   |         |          |         |              |                    |
   |         |          |         +-- tools/call: get_device_telemetry
   |         |          +------------ prompt: "Analiza consumo METER-001"
   |         +----------------------- contexto + herramientas disponibles
   +--------------------------------- solicitud natural language
\end{verbatim}

\textbf{Paso 1}: Usuario solicita análisis ("¿Hay anomalías en el medidor METER-001?") \\
\textbf{Paso 2}: MCP Client consulta al LLM disponible (Ollama local) con prompt y lista de tools del MCP Server \\
\textbf{Paso 3}: LLM determina que necesita invocar \texttt{get\_device\_telemetry("METER-001", "24h")} \\
\textbf{Paso 4}: MCP Client envía JSON-RPC request al MCP Server: \texttt{\{"method": "tools/call", "params": \{"name": "get\_device\_telemetry", "arguments": \{"device\_id": "METER-001", "timerange": "24h"\}\}\}} \\
\textbf{Paso 5}: MCP Server ejecuta consulta a ThingsBoard Edge API obteniendo 96 puntos de telemetría (intervalo 15 min) \\
\textbf{Paso 6}: MCP Server retorna datos estructurados en JSON al MCP Client \\
\textbf{Paso 7}: LLM analiza datos, detecta pico de consumo 10× superior al promedio a las 3 AM \\
\textbf{Paso 8}: MCP Client presenta respuesta interpretada al usuario: "Anomalía detectada: consumo de 500 kWh a las 3 AM (promedio normal: 50 kWh). Posible causa: bypass de medidor o falla en transformador de corriente. Se recomienda inspección física urgente."

\subsubsection{Ventajas de MCP sobre Integraciones Tradicionales}

\textbf{Desacoplamiento modelo-plataforma}: Sin MCP, cada combinación LLM×Plataforma requiere código de integración custom. Con 5 LLMs (GPT-4, Claude, Llama, Mistral, Phi-3) y 3 plataformas IoT (ThingsBoard, AWS IoT, Azure IoT Hub), se necesitarían 15 integraciones. MCP reduce esto a 5 MCP Clients + 3 MCP Servers = 8 componentes independientes, eliminando dependencias cruzadas.

\textbf{Extensibilidad}: Agregar nuevas capacidades al sistema (ej. consulta de previsiones meteorológicas, integración con ERP corporativo) solo requiere implementar un nuevo MCP Server, que automáticamente se vuelve accesible para todos los LLMs compatibles con MCP sin modificar código de cliente.

\textbf{Portabilidad de prompts y workflows}: Los flujos de análisis definidos mediante MCP tools son portables entre diferentes modelos de lenguaje. Un workflow de "detección de fraude" implementado para Ollama+Llama funciona sin cambios con Claude o GPT-4, permitiendo comparar rendimiento de modelos sin reimplementación.

\textbf{Seguridad y control de acceso}: El MCP Server actúa como capa de autorización, exponiendo únicamente las operaciones permitidas al LLM mediante tools específicos. Esto evita que el modelo ejecute operaciones no autorizadas (ej. borrado de datos, modificación de configuraciones críticas) incluso si el prompt es manipulado maliciosamente.

\textbf{Observabilidad}: Todas las invocaciones de tools son auditables mediante logs estructurados JSON-RPC, permitiendo trazabilidad completa de qué datos accedió el LLM, qué decisiones tomó y qué acciones ejecutó.

\subsection{Despliegue de Ollama: LLM Local para Edge Computing}

Ollama es una plataforma open-source que permite ejecutar modelos de lenguaje de gran tamaño localmente en hardware convencional, sin dependencias de servicios cloud. Ollama gestiona descarga de modelos, cuantización optimizada para CPU/GPU, servidor HTTP API compatible con OpenAI y gestión de contexto multi-turno. Para el gateway de Smart Energy, Ollama se despliega como contenedor Docker exponiendo puerto 11434 (API REST) con volumen persistente para almacenamiento de modelos (~2-4 GB por modelo).

\subsubsection{Selección de Modelos para Edge}

Los modelos de lenguaje se caracterizan por su tamaño en parámetros, que determina capacidad de razonamiento y requisitos de hardware:

\begin{itemize}
    \item \textbf{Llama 3.2:1b} (1 billón parámetros): Modelo ultra-ligero optimizado para edge, 1 GB RAM, inferencia <200 ms CPU, capacidad razonamiento básica, adecuado para clasificación y extracción de entidades
    \item \textbf{Llama 3.2:3b} (3 billones parámetros): Balance rendimiento/recursos, 2 GB RAM, inferencia 500 ms CPU ARM, capacidad análisis temporal y detección anomalías, **recomendado para gateway Raspberry Pi 4**
    \item \textbf{Phi-3:mini} (3.8 billones parámetros): Modelo Microsoft optimizado eficiencia, 1.3 GB cuantizado Q4\_0, especializado razonamiento matemático, excelente para análisis de series temporales energéticas
    \item \textbf{Mistral:7b} (7 billones parámetros): Alto rendimiento general, 4 GB RAM, requiere aceleración GPU para latencias <1s, análisis complejos multicontexto
\end{itemize}

Para el caso de uso de Smart Energy en gateway Raspberry Pi 4 (4 GB RAM), se recomienda **Llama 3.2:3b** o **Phi-3:mini**, que ofrecen balance óptimo entre capacidad analítica y requisitos computacionales.

\subsubsection{Configuración Docker de Ollama}

El docker-compose completo de Ollama se documenta en el \textbf{Anexo B}, incluyendo configuración de recursos (8 GB RAM limit), volúmenes persistentes (\texttt{./models:/root/.ollama}), healthcheck (ping API cada 30s) y descarga automática de modelos mediante \texttt{docker exec ollama ollama pull llama3.2:3b}.

Prueba de inferencia:
\begin{verbatim}
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "Analiza los siguientes datos de consumo energético 
             e identifica anomalías: [50, 48, 52, 500, 49, 51] kWh",
  "stream": false
}'
\end{verbatim}

Respuesta esperada (JSON):
\begin{verbatim}
{
  "response": "Se detecta una anomalía significativa en el cuarto 
               dato (500 kWh), que representa un incremento de 10× 
               respecto al patrón base de ~50 kWh...",
  "done": true,
  "context": [...],
  "total_duration": 485000000  // 485 ms
}
\end{verbatim}

\subsection{MCP Server para ThingsBoard Edge}

La implementación del MCP Server para ThingsBoard Edge expone la API REST de ThingsBoard como herramientas estructuradas invocables por el LLM, abstrayendo la complejidad de autenticación OAuth, paginación de resultados, manejo de errores HTTP y transformación de formatos de datos.

\subsubsection{Herramientas (Tools) Implementadas}

\textbf{1. \texttt{get\_device\_telemetry}} \\
\textbf{Descripción}: Obtiene series temporales de telemetría de un dispositivo específico \\
\textbf{Parámetros}:
\begin{itemize}
    \item \texttt{device\_id}: Identificador del dispositivo (ej. "METER-001")
    \item \texttt{keys}: Lista de claves de telemetría (ej. ["energy\_kwh", "voltage", "current"])
    \item \texttt{start\_ts}: Timestamp inicio (formato ISO 8601 o relativo "24h", "7d")
    \item \texttt{end\_ts}: Timestamp fin (opcional, default: now)
    \item \texttt{limit}: Máximo número de puntos (default: 100)
\end{itemize}
\textbf{Retorno}: Array de objetos \texttt{\{"ts": 1699876543000, "energy\_kwh": 123.45, "voltage": 220.3\}}

\textbf{2. \texttt{get\_device\_alarms}} \\
\textbf{Descripción}: Consulta alarmas activas o históricas de un dispositivo \\
\textbf{Parámetros}:
\begin{itemize}
    \item \texttt{device\_id}: Identificador del dispositivo
    \item \texttt{status}: Filtro de estado ("ACTIVE", "CLEARED", "ACK", "ALL")
    \item \texttt{severity}: Filtro de severidad ("CRITICAL", "MAJOR", "MINOR", "WARNING")
    \item \texttt{limit}: Máximo número de alarmas (default: 50)
\end{itemize}
\textbf{Retorno}: Array de objetos con tipo de alarma, timestamp, severidad y mensaje

\textbf{3. \texttt{get\_device\_attributes}} \\
\textbf{Descripción}: Obtiene atributos estáticos o compartidos de un dispositivo \\
\textbf{Parámetros}:
\begin{itemize}
    \item \texttt{device\_id}: Identificador del dispositivo
    \item \texttt{scope}: Alcance de atributos ("SERVER\_SCOPE", "SHARED\_SCOPE", "CLIENT\_SCOPE")
    \item \texttt{keys}: Lista opcional de claves específicas
\end{itemize}
\textbf{Retorno}: Diccionario de atributos clave-valor (ej. \texttt{\{"lat": 4.8156, "lon": -75.6942, "firmware": "v2.1.3"\}})

\subsubsection{Protocolo JSON-RPC 2.0}

El MCP Server implementa JSON-RPC 2.0 sobre stdio (stdin/stdout) para comunicación con el MCP Client. Ejemplo de intercambio:

\textbf{Request (Client → Server):}
\begin{verbatim}
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/call",
  "params": {
    "name": "get_device_telemetry",
    "arguments": {
      "device_id": "METER-001",
      "keys": ["energy_kwh"],
      "start_ts": "24h",
      "limit": 100
    }
  }
}
\end{verbatim}

\textbf{Response (Server → Client):}
\begin{verbatim}
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "content": [
      {
        "type": "text",
        "text": "{\"data\": [{\"ts\": 1699876543000, \"energy_kwh\": 123.45}, 
                   {\"ts\": 1699880143000, \"energy_kwh\": 125.67}, ...]}"
      }
    ]
  }
}
\end{verbatim}

El código completo del MCP Server \texttt{tb\_mcp\_server.py} (Python + requests + sys.stdin loop) y configuración MCP Client \texttt{mcp\_config.json} se documentan en el \textbf{Anexo C}.

\subsection{Casos de Uso de IA en Smart Energy}

\subsubsection{1. Detección de Fraude Eléctrico}

\textbf{Contexto}: El fraude eléctrico (bypass de medidores, manipulación de transformadores de corriente) genera pérdidas de \$1,500-3,000 millones anuales a empresas distribuidoras en Colombia. La detección tradicional requiere inspecciones físicas costosas con tasa de acierto <40\%.

\textbf{Prompt al LLM}:
\begin{verbatim}
"Analiza el consumo del medidor METER-001 en las últimas 24 horas 
e identifica patrones anómalos que pudieran indicar fraude eléctrico, 
específicamente: (1) caídas súbitas >80% del consumo base, 
(2) picos nocturnos >10× promedio, (3) desviaciones de perfil de carga 
respecto a tendencia histórica de 30 días."
\end{verbatim}

\textbf{Flujo de ejecución}: \\
1. LLM invoca \texttt{get\_device\_telemetry("METER-001", ["energy\_kwh"], "24h", 96)} obteniendo datos cada 15 min \\
2. LLM invoca \texttt{get\_device\_telemetry("METER-001", ["energy\_kwh"], "30d", 200)} para baseline histórico \\
3. LLM calcula estadísticas: media 48.5 kWh, desviación estándar 5.2 kWh, detecta punto a las 3:00 AM con 485 kWh (9.4 desviaciones estándar) \\
4. LLM genera diagnóstico:

\textbf{Respuesta generada}:
\begin{verbatim}
"ALERTA CRÍTICA: Anomalía detectada en METER-001
- Timestamp: 2024-11-07 03:15:00
- Consumo registrado: 485 kWh (desviación 9.4σ del baseline)
- Patrón histórico: 45-52 kWh (media 48.5 kWh)
- Diagnóstico: Posible bypass del medidor o falla en CT primario
- Confianza: 94% (basado en análisis de 2880 puntos históricos)
- Recomendación: Inspección física urgente en <24h
- Pérdida estimada si fraude confirmado: $1,200-1,800 USD/mes"
\end{verbatim}

\textbf{Impacto}: Reducción 70\% de inspecciones innecesarias, incremento 85\% en tasa de acierto de fraudes detectados, ROI positivo en <6 meses considerando ahorro en pérdidas no técnicas.

\subsubsection{2. Mantenimiento Predictivo de Infraestructura}

\textbf{Contexto}: Fallas en medidores y transformadores generan interrupciones de servicio (SAIDI >12 horas/año en redes rurales colombianas) y costos de reemplazo reactivo elevados.

\textbf{Prompt al LLM}:
\begin{verbatim}
"Evalúa el estado operativo de los 50 medidores de la zona Norte 
y genera un ranking de prioridad para mantenimiento preventivo 
en los próximos 30 días. Considera: (1) cantidad de alarmas activas, 
(2) varianza de voltaje >5% respecto a nominal, 
(3) edad del dispositivo >5 años, (4) historial de reinicios >3/mes."
\end{verbatim}

\textbf{Flujo de ejecución}: \\
1. LLM itera sobre 50 dispositivos invocando \texttt{get\_device\_alarms} y \texttt{get\_device\_attributes} \\
2. LLM consulta telemetría de voltaje con \texttt{get\_device\_telemetry} calculando varianza para cada dispositivo \\
3. LLM asigna score de riesgo ponderado: alarmas (40\%), varianza voltaje (30\%), edad (20\%), reinicios (10\%) \\
4. LLM ordena dispositivos por score y genera reporte:

\textbf{Respuesta generada}:
\begin{verbatim}
"REPORTE MANTENIMIENTO PREDICTIVO - Zona Norte (50 dispositivos)

PRIORIDAD CRÍTICA (intervenir en 7 días):
1. METER-042 [Score: 87/100]
   - 15 alarmas activas (bajo voltaje, high temperature)
   - Varianza voltaje 8.3% (spec: <5%)
   - 7 reinicios en últimos 30 días
   - Edad: 6.2 años
   - Predicción: Falla inminente transformador interno (probabilidad 78%)
   
2. METER-089 [Score: 81/100]
   - 12 alarmas activas
   - Varianza voltaje 7.1%
   ...

PRIORIDAD ALTA (intervenir en 15 días):
3. METER-123 [Score: 72/100]
   ...

Total dispositivos analizados: 50
Dispositivos prioridad crítica: 2 (4%)
Dispositivos prioridad alta: 5 (10%)
Costo estimado mantenimiento preventivo: $1,400 USD
Ahorro vs reemplazo reactivo: $8,200 USD (ROI 5.9×)"
\end{verbatim}

\textbf{Impacto}: Reducción 60\% en tiempo de inactividad no planificado, extensión 25\% vida útil de equipos mediante mantenimiento oportuno, optimización de rutas de técnicos (+35\% eficiencia operativa).

\subsubsection{3. Optimización de Respuesta a la Demanda (Demand Response)}

\textbf{Contexto}: Los programas de respuesta a la demanda permiten reducir picos de consumo en horarios críticos (6-10 PM) mediante incentivos tarifarios, reduciendo necesidad de generación de punta (costosa y contaminante).

\textbf{Prompt al LLM}:
\begin{verbatim}
"Analiza el perfil de consumo de los 200 medidores residenciales 
en las últimas 7 días. Identifica los 20 clientes con mayor consumo 
en horario pico (6-10 PM) y estima el potencial de reducción de carga 
si se les ofrece tarifa diferencial de $0.15/kWh (vs $0.28/kWh pico). 
Calcula el impacto en peak shaving total."
\end{verbatim}

\textbf{Respuesta generada}:
\begin{verbatim}
"ANÁLISIS DEMAND RESPONSE - 200 medidores residenciales

TOP 20 CONSUMIDORES HORARIO PICO (6-10 PM):
1. METER-156: 12.8 kWh/día pico (28% consumo total diario)
2. METER-203: 11.4 kWh/día pico (25% consumo total diario)
...

POTENCIAL PEAK SHAVING:
- Consumo pico actual agregado: 245 kW (6:30 PM promedio)
- Reducción estimada con DR program: 68 kW (27.8%)
- Consumo pico proyectado post-DR: 177 kW
- Evitación generación punta: 68 kW × 120 días/año = 8,160 kWh/año
- Ahorro CO2: 3.2 ton/año (factor emisión 0.39 kg CO2/kWh Colombia)
- Costo incentivos clientes: $2,040/año
- Ahorro evitación punta: $9,800/año (tarifa generación pico $1.20/kWh)
- ROI: 4.8× (recuperación <3 meses)"
\end{verbatim}

\textbf{Impacto}: Reducción 25-35\% en picos de demanda, postergación inversión en ampliación de subestaciones (\$1.2-2.5 millones), reducción huella de carbono, mejora estabilidad de red.

\subsection{Ventajas de IA Local vs IA Cloud}

\begin{table}[h]
\centering
\caption{Comparativa IA Local (Gateway Ollama) vs IA Cloud (GPT-4/Claude)}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Característica} & \textbf{IA Local (Ollama)} & \textbf{IA Cloud (GPT-4)} \\
\hline
Latencia & <500 ms & 2-5 segundos \\
Privacidad & Alta (datos locales) & Baja (envío cloud) \\
Costo operativo & \$0 (hardware local) & \$0.01-0.10/consulta \\
Disponibilidad offline & 100\% & 0\% (requiere WAN) \\
Modelos disponibles & Open-source (Llama, Phi-3) & Propietarios (GPT-4) \\
Capacidad análisis & Media (3B-7B params) & Alta (100B+ params) \\
Consumo energético & +5W CPU / +15W GPU & N/A (cloud) \\
Escalabilidad & Distribuida (por gateway) & Centralizada (cloud) \\
Cumplimiento normativo & Total (datos no salen) & Parcial (DPA agreements) \\
\hline
\end{tabular}
\end{table}

\textbf{Recomendación arquitectónica}: Implementar arquitectura híbrida con IA local para análisis en tiempo real (detección fraude, alarmas críticas, disponibilidad 24/7 offline) y reservar IA cloud para análisis complejos periódicos (optimización de red semanal/mensual, tendencias macroeconómicas, previsiones long-term) que requieren capacidad de razonamiento superior y pueden tolerar latencias >5 segundos. Esta estrategia optimiza balance costo/rendimiento/privacidad.

\section{Conclusiones del Capítulo}

El gateway basado en OpenWRT con arquitectura de contenedores Docker y conectividad multiradio (HaLow + LTE) ofrece ventajas significativas para despliegues Smart Energy:

\begin{itemize}
    \item \textbf{Flexibilidad}: Contenedores Docker permiten actualizar/escalar servicios independientemente
    \item \textbf{Edge Computing}: ThingsBoard Edge procesa datos localmente reduciendo latencia y dependencia cloud
    \item \textbf{Conectividad robusta multimodal}: HaLow (Morse Micro MM6108) 1-3 km hasta 40 Mbps con 4 modos (AP/STA/Mesh/EasyMesh) + LTE Cat-6 redundante con failover <30s
    \item \textbf{Escalabilidad Arquitectónica}: Estrella (2,500 endpoints / 3 km), Mesh 802.11s (7,500 endpoints / 9 km auto-healing), EasyMesh (12,500 endpoints / roaming transparente)
    \item \textbf{Reducción CAPEX/OPEX}: Mesh 66\% ahorro infraestructura WAN, \$3,240/año ahorro planes LTE con backhaul HaLow sin costo recurrente
    \item \textbf{Interoperabilidad}: OpenThread Border Router con soporte Thread 1.3 multi-vendor compatible
    \item \textbf{Resiliencia}: SSD NVMe (>1M ciclos E/W, >3000 IOPS, <0.1ms latencia), queue persistente TB Edge (100k msgs, 2 GB, sincronización catch-up <15 min con batch 5000 + gzip), 6 niveles resiliencia hardware/filesystem/DB/aplicación/red/containers (RTO <5 min), mesh auto-healing (<10s reconvergencia HWMP eliminando single point of failure)
    \item \textbf{Inteligencia Artificial (Roadmap Futuro)}: MCP + Ollama para análisis local (latencia <500 ms, privacidad 100\% datos no salen), requiere optimización térmica RPi 4, alternativa servidor dedicado para análisis batch offline
    \item \textbf{Arquitectura de Datos Distribuida}: Kafka (>100k msg/s, buffer 7 días, replay histórico, multi-consumidor, backpressure), PostgreSQL+TimescaleDB (compresión 10-20×, particionamiento automático, >3000 IOPS en NVMe, aggregaciones time\_bucket)
    \item \textbf{Protocolos Multiprotocolo}: MQTT (QoS 0/1/2 Pub/Sub), CoAP (UDP 4 bytes overhead Observe), HTTP/REST (APIs gestión), LwM2M (OTA firmware, objetos OMA estándar, DTLS eficiente PSK 16B vs X.509 2KB)
    \item \textbf{Seguridad multicapa}: Firewall nftables (puertos explícitos), container isolation (namespaces), TLS/mTLS cloud (puerto 7070 gRPC), Thread AES-128-CCM, HaLow WPA3-SAE+PMF (Morse Micro), OpenVPN (túnel permanente NOC sin exponer puertos internet)
    \item \textbf{Mantenibilidad}: OpenWRT Feeds (opkg custom packages Smart Grid), OpenVPN (túnel VPN permanente hub-spoke IPs fijas 10.8.0.100-199), OpenWISP (gestión masiva 100-1000 GWs templates UCI push remoto, Firmware OTA scheduler dual-partition rollback, monitoring CPU/RAM/Interfaces/Docker alertas email/SMS), Watchtower (OTA contenedores), backups automatizados cron
    \item \textbf{Escalabilidad}: 10 DCUs × 250 nodos Thread = 2,500 endpoints AP. Mesh/EasyMesh multiplican 3-5× capacidad sin rediseño arquitectónico
    \item \textbf{Costo-efectividad}: Hardware propósito general (router OpenWRT + módulos M.2 estándar) reduce CAPEX vs propietarios, optimización LTE 3.7 GB/mes (vs 20-30 GB sin compresión CBOR 40-60\%), Mesh HaLow elimina 60-70\% backhaul dedicado
    \item \textbf{Conformidad Estándares}: IEEE 2030.5-2023 (Function Sets DCAP/TM/MM/MSG/ED, API REST XML, X.509 ECC P-256, LFDI, RBAC), ISO/IEC 30141:2024 (arquitectura IoT referencia 8 entidades funcionales, 4 vistas funcional/información/despliegue/operacional), cumplimiento regulatorio CREG Colombia para medición inteligente
\end{itemize}

\subsection{Limitaciones y Trabajo Futuro}

Validación performance (mediciones CPU/RAM bajo carga completa, benchmarks temperatura con ventilador activo objetivo <75°C, test throughput E2E nodo Thread → OTBR → HaLow → TB Edge → PostgreSQL, stress test 1000 msg/s durante 24h validar estabilidad térmica y resiliencia SSD), conectividad HaLow via USB (Morse Micro Q2 2026 USB 2.0 High-Speed simplifica integración elimina complejidad SPI), IA local (Ollama Llama 3.2 1B o Phi-3 mini en RPi 4 8 GB RAM, validar casos uso detección anomalías fraude bypass CT y mantenimiento predictivo ranking dispositivos alarmas, alternativa Ollama servidor x86 para análisis batch offline datos PostgreSQL), rendimiento I/O (RAID-1 NVMe para >500 dispositivos requiere Compute Module 4 dual M.2), alta disponibilidad (par gateways RPi 4 activo-pasivo VRRP/keepalived, en mesh configurar 2 gateways uplink LTE root bridges redundantes RSTP), RPi vs hardware industrial (migración CM4 carrier board DIN-rail -40°C a +85°C dual Ethernet dual M.2 NVMe certificaciones industriales vibración EMI/EMC, alternativa x86 industrial Intel Atom/Celeron N5105 8 GB RAM dual NIC PCIe mayor costo \$200-300 vs \$55 RPi 4), 5G RedCap (Quectel RG500U latencia <50ms vs 100-300ms LTE-M throughput 100 Mbps vs 375 kbps crítico comandos RPC downlink tiempo real), agregación enlaces (MPTCP Ethernet+LTE simultáneos failover <1s sin pérdida TCP), mesh avanzado (802.11r fastroaming <50ms EasyMesh handoff crítico vehículos eléctricos movimiento carga dinámica V2G), HaLow+LoRaWAN híbrido (sensores ultra-low-power <10 mW batería 10 años LoRaWAN 915 MHz con HaLow backhaul gateways LoRa concentradores Semtech SX1302), quantum-safe crypto (algoritmos post-cuánticos Kyber-768 Dilithium-3 en certificados X.509 protección largo plazo NIST PQC Round 4 2025+ crítico infraestructura Smart Grid vida útil >20 años).

\textbf{Próximo capítulo}: Arquitectura completa del sistema integrando nodos Thread (ESP32-C6), DCUs con Thread Border Router, gateway Raspberry Pi 4 + OpenWRT con HaLow multimodal (AP/STA/Mesh/EasyMesh), Quectel BG95 LTE-M y nRF52840 Thread RCP, y plataforma cloud ThingsBoard, con caso de estudio de despliegue real para 900 medidores residenciales en infraestructura colombiana con topología mesh 802.11s (3 gateways × 9 km cobertura × 300 medidores por gateway).
